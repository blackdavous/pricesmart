# An√°lisis de Integraci√≥n - ML API + Web Scraping

## üîç Resultados de las Pruebas

### Estado del Token ‚úÖ
- **Token renovado exitosamente**
- User: ROAL2642779 (ID: 491630401)
- V√°lido por: 6 horas
- Auto-renovaci√≥n disponible con refresh_token

### Tests de Conectividad

| Test | Estado | Notas |
|------|--------|-------|
| Token Validity | ‚úÖ PASS | `/users/me` funciona |
| Search Endpoint | ‚ùå FAIL (403) | Acceso prohibido - cuenta puede estar limitada |
| Categories | ‚úÖ PASS | `/categories` funciona sin problemas |

---

## üí° Conclusi√≥n Clave

**La API tiene restricciones 403 en b√∫squedas**, probablemente porque:
1. Cuenta nueva sin historial de ventas
2. Rate limiting preventivo de Mercado Libre
3. Scopes insuficientes para b√∫squedas p√∫blicas

**PERO** el notebook de Gustavo funciona perfecto porque:
- ‚úÖ Hace web scraping del HTML p√∫blico
- ‚úÖ No requiere autenticaci√≥n
- ‚úÖ Extrae datos directamente del frontend
- ‚úÖ M√°s robusto para este caso de uso

---

## üéØ Estrategia H√≠brida Recomendada

### Arquitectura de 3 Capas

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         MarketResearchAgent                     ‚îÇ
‚îÇ      (Orquestador de b√∫squeda)                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                   ‚Üì
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ   Data Strategy   ‚îÇ
         ‚îÇ   Selector        ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                   ‚Üì
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚Üì              ‚Üì              ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Layer 1‚îÇ    ‚îÇ Layer 2  ‚îÇ   ‚îÇ Layer 3  ‚îÇ
‚îÇWeb Scrp‚îÇ    ‚îÇ ML API   ‚îÇ   ‚îÇ Cache    ‚îÇ
‚îÇ PRIMARY‚îÇ    ‚îÇ FALLBACK ‚îÇ   ‚îÇ OPTIONAL ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Layer 1: Web Scraper (PRIMARY)
**Basado en el notebook de Gustavo**

‚úÖ **Ventajas:**
- Funciona sin restricciones
- Extrae datos ricos del HTML
- No consume rate limits
- An√°lisis estad√≠stico integrado (IQR)

üìù **Implementaci√≥n:**
```python
# backend/app/mcp_servers/mercadolibre/scraper.py
class MLWebScraper:
    def search_products_html(query: str) -> List[Offer]:
        """Extrae productos del HTML (notebook logic)"""
        # 1. Extract __PRELOADED_STATE__
        # 2. Fallback to JSON-LD
        # 3. Filter by product matching
        # 4. Apply IQR outlier detection
```

### Layer 2: ML API (FALLBACK)
**Para endpoints que S√ç funcionan**

‚úÖ **Endpoints disponibles:**
- `/users/me` - Info de usuario
- `/categories` - Categor√≠as
- `/items/{id}` - Detalles de producto (si tenemos ID)

‚ùå **Endpoints bloqueados:**
- `/sites/MLM/search` - 403 Forbidden

üìù **Uso limitado:**
```python
# Solo para obtener categor√≠as y detalles espec√≠ficos
async def get_category_id(category_name: str) -> str:
    """Usar API para mapear categor√≠as"""
    
async def get_item_details(item_id: str) -> dict:
    """Si tenemos un ID, obtener detalles completos"""
```

### Layer 3: Cache (OPTIONAL)
**Redis para evitar re-scrapear**

```python
# Cache por 6 horas
cache_key = f"ml:search:{query_hash}"
if cached := redis.get(cache_key):
    return cached
```

---

## üöÄ Plan de Implementaci√≥n

### Fase 1: Migrar L√≥gica del Notebook ‚úÖ PRIORITARIO

**Archivos a crear/actualizar:**

1. **`backend/app/mcp_servers/mercadolibre/scraper.py`**
   - Clase `MLWebScraper` con toda la l√≥gica del notebook
   - M√©todos: `extract_preloaded_state`, `extract_jsonld`, `build_offers`

2. **`backend/app/mcp_servers/mercadolibre/models.py`**
   - Dataclasses: `IdentifiedProduct`, `Offer`, `PriceStatistics`
   - Funciones de normalizaci√≥n y matching

3. **`backend/app/mcp_servers/mercadolibre/stats.py`**
   - An√°lisis estad√≠stico (IQR, percentiles, outliers)
   - Funciones del notebook: `percentile`, `iqr_bounds`, `summarize_offers`

4. **Actualizar `backend/app/mcp_servers/mercadolibre/server.py`**
   - Integrar `MLWebScraper` como m√©todo principal
   - Mantener `MercadoLibreClient` para endpoints que funcionan

### Fase 2: Integrar con Agentes

**Actualizar `backend/app/agents/market_research.py`:**

```python
async def execute_searches(self, state: MarketResearchState):
    """
    Usa web scraper en lugar de API search.
    """
    from app.mcp_servers.mercadolibre.scraper import MLWebScraper
    
    scraper = MLWebScraper()
    
    for query in state["search_queries"]:
        # Use scraper instead of API
        result = await scraper.search_products_html(
            query=" ".join(query.keywords)
        )
        all_results.extend(result["offers"])
    
    state["raw_results"] = all_results
    return state
```

### Fase 3: Token Management

**Para endpoints que S√ç funcionan (categor√≠as, detalles):**

```python
# backend/app/mcp_servers/mercadolibre/token_manager.py
class MLTokenManager:
    def __init__(self):
        self.token_path = "ml_token.json"
        self.refresh_threshold = 3600  # Renovar 1h antes
    
    async def get_valid_token(self) -> str:
        """Auto-renovar si est√° por expirar"""
        token = self.load_token()
        if self.is_near_expiry(token):
            token = await self.refresh_token()
        return token["access_token"]
```

---

## üìã Checklist de Migraci√≥n

### Inmediato (Hoy)
- [x] Probar token ML
- [x] Identificar limitaciones (403 en search)
- [ ] Extraer c√≥digo del notebook a m√≥dulos Python
- [ ] Crear `scraper.py` con l√≥gica del notebook
- [ ] Tests unitarios del scraper

### Corto Plazo (Esta Semana)
- [ ] Integrar scraper con `MarketResearchAgent`
- [ ] Agregar logging detallado
- [ ] Implementar token auto-renewal
- [ ] Cache con Redis (opcional)
- [ ] Tests end-to-end

### Mediano Plazo (Pr√≥ximas 2 Semanas)
- [ ] Monitoring de rate limits (scraping)
- [ ] Retry logic con exponential backoff
- [ ] User-agent rotation (evitar detecci√≥n)
- [ ] Proxy support (si es necesario)
- [ ] Dashboard de m√©tricas

---

## ‚ö†Ô∏è Consideraciones Importantes

### Web Scraping - Buenas Pr√°cticas

```python
# 1. Respetar robots.txt
# 2. Rate limiting (1-2 requests por segundo)
time.sleep(random.uniform(1.0, 2.0))

# 3. User-Agent realista (ya implementado en notebook)
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) ...",
    "Accept-Language": "es-MX,es;q=0.9"
}

# 4. Manejo de errores
try:
    response = session.get(url, timeout=25)
except requests.exceptions.HTTPError as e:
    if e.response.status_code == 429:
        # Backoff exponencial
```

### Prevenci√≥n de Ban

1. **Rate Limiting:**
   - Max 30-60 requests/minuto
   - Pausas aleatorias entre requests

2. **Session Persistence:**
   - Reusar sesi√≥n HTTP (ya en notebook)
   - Cookies persistence

3. **Monitoring:**
   - Log todos los status codes
   - Alert si detectamos bloqueos

---

## üéâ Conclusi√≥n

**Mejor estrategia para tu proyecto:**

```
‚úÖ USAR Web Scraping (notebook) como PRINCIPAL
‚úÖ API solo para categor√≠as y detalles de items
‚úÖ Token management autom√°tico
‚úÖ Cache para optimizar
‚ùå NO usar /search de API (est√° bloqueado)
```

**Pr√≥ximo paso:**
Migrar el c√≥digo del notebook a `backend/app/mcp_servers/mercadolibre/scraper.py`

¬øProcedemos con la migraci√≥n del notebook a m√≥dulos Python del backend?
